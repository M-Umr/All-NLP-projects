{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fahim/Software/miniconda3/envs/dl4nlp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:\n",
      "Train: 431408\n",
      "Dev: 124528\n",
      "Test: 136864\n",
      "﻿ __newline__ Project Gutenberg’s The Complete Works of William Shakespeare, by William __newline__ \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "NEWLINE_TOKEN = ' __newline__ '\n",
    "UNK_TOKEN = '__unk__'\n",
    "\n",
    "# Read and collect text\n",
    "train_text = \"\"\n",
    "dev_text = \"\"\n",
    "test_text = \"\"\n",
    "texts = [train_text, dev_text, test_text]\n",
    "\n",
    "for text_idx, file in enumerate(['./data/shakespeare/train.txt', './data/shakespeare/val.txt', './data/shakespeare/test.txt']):\n",
    "    with open(file, 'r') as fp:\n",
    "        texts[text_idx] += NEWLINE_TOKEN.join([l.strip() for l in fp.readlines()]) + NEWLINE_TOKEN\n",
    "\n",
    "train_text, dev_text, test_text = texts\n",
    "\n",
    "print(\"Total characters:\")\n",
    "print(\"Train: %d\"%(len(train_text)))\n",
    "print(\"Dev: %d\"%(len(dev_text)))\n",
    "print(\"Test: %d\"%(len(test_text)))\n",
    "print(train_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text\n",
    "We usually preprocess the text to remove casing information, separate out punctuations etc to make our data cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens:\n",
      "Train: 81356\n",
      "Dev: 23823\n",
      "Test: 27236\n"
     ]
    }
   ],
   "source": [
    "tokens = [None, None, None]\n",
    "for text_idx in range(len(texts)):\n",
    "    tokens[text_idx] = word_tokenize(texts[text_idx].lower())\n",
    "\n",
    "train_tokens, dev_tokens, test_tokens = tokens\n",
    "\n",
    "print(\"Total tokens:\")\n",
    "print(\"Train: %d\"%(len(train_tokens)))\n",
    "print(\"Dev: %d\"%(len(dev_tokens)))\n",
    "print(\"Test: %d\"%(len(test_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6650\n",
      "Most frequent tokens\n",
      "\t__newline__: 10000\n",
      "\t,: 5218\n",
      "\t.: 4361\n",
      "\tthe: 1658\n",
      "\tand: 1456\n",
      "\ti: 1414\n",
      "\tto: 1254\n",
      "\t’: 1186\n",
      "\tof: 1111\n",
      "\tmy: 906\n",
      "Least frequent tokens\n",
      "\timpossible-: 1\n",
      "\tdescried: 1\n",
      "\tapproaching: 1\n",
      "\tfull-mann: 1\n",
      "\tsixty: 1\n",
      "\tsecurity: 1\n",
      "\tassurance: 1\n",
      "\tforgo: 1\n",
      "\trenowned: 1\n",
      "\tunexecuted: 1\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "full_vocab = dict()\n",
    "for token in train_tokens:\n",
    "    full_vocab[token] = full_vocab.get(token, 0) + 1\n",
    "\n",
    "# Sort vocabulary by occurence\n",
    "sorted_vocab = sorted(full_vocab.keys(), key=lambda word: -full_vocab[word])\n",
    "\n",
    "# Print some samples\n",
    "print(\"Vocabulary size: %d\"%(len(sorted_vocab)))\n",
    "print(\"Most frequent tokens\")\n",
    "for i in range(10):\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[i], full_vocab[sorted_vocab[i]]))\n",
    "print(\"Least frequent tokens\")\n",
    "for i in range(1,11):\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[-i], full_vocab[sorted_vocab[-i]]))\n",
    "\n",
    "# Create final vocab\n",
    "word2idx = {w: idx for idx, w in enumerate(sorted_vocab[:VOCAB_SIZE])}\n",
    "idx2word = {idx: w for idx, w in enumerate(sorted_vocab[:VOCAB_SIZE])}\n",
    "\n",
    "word2idx[UNK_TOKEN] = VOCAB_SIZE\n",
    "idx2word[VOCAB_SIZE] = UNK_TOKEN\n",
    "VOCAB_SIZE = VOCAB_SIZE + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter text based on vocabulary\n",
    "We will now have to replace words we do not have in the vocabulary with a special token, `__unk__` in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens filtered out as unknown:\n",
      "Train: 1650/81356\n",
      "Dev: 1946/23823\n",
      "Test: 2752/27236\n"
     ]
    }
   ],
   "source": [
    "for tokens_idx in range(len(tokens)):\n",
    "    tokens[tokens_idx] = [t if t in word2idx else UNK_TOKEN for t in tokens[tokens_idx]]\n",
    "\n",
    "train_tokens, dev_tokens, test_tokens = tokens\n",
    "print(\"Number of tokens filtered out as unknown:\")\n",
    "print(\"Train: %d/%d\"%(len([1 for t in train_tokens if t == UNK_TOKEN]), len(train_tokens)))\n",
    "print(\"Dev: %d/%d\"%(len([1 for t in dev_tokens if t == UNK_TOKEN]), len(dev_tokens)))\n",
    "print(\"Test: %d/%d\"%(len([1 for t in test_tokens if t == UNK_TOKEN]), len(test_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data in tensor form\n",
    "Our keras models finally take tensors as input and labels, so we need to modify our data to fit this form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([word2idx[t] for t in train_tokens])\n",
    "X_dev = np.array([word2idx[t] for t in dev_tokens])\n",
    "X_test = np.array([word2idx[t] for t in test_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels in this exercise are just the next words. Hence, for\n",
    "\n",
    ">   `X_train = ['hello', 'how', 'are', 'you', '?']`\n",
    "\n",
    "we will have:\n",
    "\n",
    ">    `y_train = ['how, 'are', you', '?']`\n",
    "\n",
    "Which is just `X_train[1:]`\n",
    "We will also remove the last element of `X_train`, since we do not have any label for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "def build_bag_of_words(X, context_size=1, vocab_size=VOCAB_SIZE):\n",
    "    num_examples = X.shape[0]-context_size\n",
    "    X_bow = np.zeros((num_examples, vocab_size))\n",
    "    \n",
    "    y_bow = np.zeros((num_examples, vocab_size))\n",
    "    \n",
    "    for idx in range(num_examples):\n",
    "        for context_idx in range(context_size):\n",
    "            X_bow[idx, X[idx+context_idx]] = 1\n",
    "        y_bow[idx, X[idx + context_size]] = 1\n",
    "    \n",
    "    return X_bow, y_bow\n",
    "            \n",
    "def get_next_predicted_word(model, input_words, context_size=1):\n",
    "    if not isinstance(input_words, list):\n",
    "        input_words = [input_words]\n",
    "    input_words = input_words + [\"__unk__\"]\n",
    "    input_array = np.array([word2idx[w] for w in input_words])\n",
    "    input_bow, _ = build_bag_of_words(input_array, context_size=context_size)\n",
    "    scores = model.predict(input_bow)\n",
    "    output_word = idx2word[np.argmax(scores)]\n",
    "    \n",
    "    return output_word\n",
    "\n",
    "def get_sentence(model, start_words, context_size=1):\n",
    "    if not isinstance(start_words, list):\n",
    "        start_words = [start_words]\n",
    "\n",
    "    output = [] + start_words\n",
    "    while output[-1] != '__newline__' and len(output) < 100:\n",
    "        prev_word = get_next_predicted_word(model, output[-context_size:], context_size=context_size)\n",
    "        output.append(prev_word)\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram, y_train_bigram = build_bag_of_words(X_train, context_size=1)\n",
    "X_dev_bigram, y_dev_bigram = build_bag_of_words(X_dev, context_size=1)\n",
    "X_test_bigram, y_test_bigram = build_bag_of_words(X_test, context_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81355, 5001)\n",
      "(23822, 5001)\n",
      "(27235, 5001)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bigram.shape)\n",
    "print(X_dev_bigram.shape)\n",
    "print(X_test_bigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_amd_radeon_pro_560.0\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               500200    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5001)              505101    \n",
      "=================================================================\n",
      "Total params: 1,025,501\n",
      "Trainable params: 1,025,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(VOCAB_SIZE,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81355 samples, validate on 23822 samples\n",
      "Epoch 1/1\n",
      "81355/81355 [==============================] - 32s 399us/step - loss: 5.6631 - acc: 0.1486 - val_loss: 5.1469 - val_acc: 0.1344\n",
      "i , __newline__\n",
      "Train on 81355 samples, validate on 23822 samples\n",
      "Epoch 2/2\n",
      "81355/81355 [==============================] - 37s 457us/step - loss: 5.1015 - acc: 0.1936 - val_loss: 4.9344 - val_acc: 0.1644\n",
      "i have , __newline__\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.fit(X_train_bigram, y_train_bigram, batch_size=128, epochs=epoch+1, initial_epoch=epoch, validation_data=(X_dev_bigram, y_dev_bigram))\n",
    "    print(get_sentence(model, ['i']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think __newline__\n",
      "well , __newline__\n",
      "i have , __newline__\n",
      "who , __newline__\n"
     ]
    }
   ],
   "source": [
    "print(get_sentence(model, ['think']))\n",
    "print(get_sentence(model, ['well']))\n",
    "print(get_sentence(model, ['i']))\n",
    "print(get_sentence(model, ['who']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add context\n",
    "The above data uses only _one_ previous word as context, but we can change our data to include more words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trigram, y_train_trigram = build_bag_of_words(X_train, context_size=2)\n",
    "X_dev_trigram, y_dev_trigram = build_bag_of_words(X_dev, context_size=2)\n",
    "X_test_trigram, y_tes_trigram = build_bag_of_words(X_test, context_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81354, 5001)\n",
      "(81354, 5001)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trigram.shape)\n",
    "print(y_train_trigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 100)               500200    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5001)              505101    \n",
      "=================================================================\n",
      "Total params: 1,025,501\n",
      "Trainable params: 1,025,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_trigram = Sequential()\n",
    "model_trigram.add(Dense(100, input_shape=(VOCAB_SIZE,)))\n",
    "model_trigram.add(Dense(100, activation='relu'))\n",
    "model_trigram.add(Dense(100, activation='relu'))\n",
    "model_trigram.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "\n",
    "model_trigram.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_trigram.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81354 samples, validate on 23821 samples\n",
      "Epoch 1/1\n",
      "81354/81354 [==============================] - 40s 493us/step - loss: 5.7229 - acc: 0.1427 - val_loss: 5.2048 - val_acc: 0.1353\n",
      "i have , __newline__\n",
      "Train on 81354 samples, validate on 23821 samples\n",
      "Epoch 2/2\n",
      "81354/81354 [==============================] - 38s 464us/step - loss: 5.1496 - acc: 0.1850 - val_loss: 5.0672 - val_acc: 0.1468\n",
      "i have have , __newline__\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_trigram.fit(X_train_trigram, y_train_trigram, batch_size=128, epochs=epoch+1, initial_epoch=epoch, validation_data=(X_dev_trigram, y_dev_trigram))\n",
    "    print(get_sentence(model_trigram, ['i', 'have'], context_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think of , __newline__\n",
      "well we , the __unk__ of , __newline__\n",
      "i have have , __newline__\n",
      "who will the __unk__ of , __newline__\n"
     ]
    }
   ],
   "source": [
    "print(get_sentence(model_trigram, ['think', 'of'], context_size=2))\n",
    "print(get_sentence(model_trigram, ['well', 'we'], context_size=2))\n",
    "print(get_sentence(model_trigram, ['i', 'have'], context_size=2))\n",
    "print(get_sentence(model_trigram, ['who', 'will'], context_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
